"""
JSON Schema Generator for UIA Listener Data
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional
import sys

sys.path.append(str(Path(__file__).parent.parent))
from open_ai.openai_client import run_request

JSON_SCHEMA_PROMPT = """
You are a senior QA automation engineer. From raw UIA listener events and examples,
produce a STRICT JSON Schema (Draft 2020-12) that validates an ARRAY of UIA events.

From (1) raw UIA listener events and (2) examples, you will:
  A) Produce a STRICT JSON Schema (Draft 2020-12) that validates an ARRAY of UIA events ("event_schema").
  B) Produce a STRICT JSON Schema (Draft 2020-12) that validates an ACTION PLAN ("action_schema") using the provided action formats.
  C) Generate a minimal ACTION PLAN ("actions") that maps the given raw events to concrete actions (1:1 or many:1 where sensible), referencing targets defined under "targets".

Your output MUST be deterministic, strictly valid JSON, and compile under Draft 2020-12.

## Inputs
<bot_listener_data>
{bot_listener_data}
</bot_listener_data>

<listener_data_examples>
{listener_data_examples}
</listener_data_examples>

<actions_formats>
{actions_formats}
</actions_formats>

## Objectives
- Infer a robust "event_schema" for UIA events that appear in the input and examples.
- Infer a robust "action_schema" aligned to the action formats provided.
- Normalize noisy UIA bursts (e.g., repeated Focus/PropertyChanged) into stable, minimal actions.
- Prefer stable selectors (controlType, className, name) and allow multiple selector variants per target.
- Capture timeouts/retries where relevant; use conditionals (if_exists) for optional UI surfaces.

## Requirements
The following requirements MUST be reflected in the generated schema:

<schema_requirements>
...
</schema_requirements>

## Deliverable
Return ONLY: {{"schema": {{...}}}}

"""

class UIA_Listener_Data:
    def __init__(self):
        script_dir = Path(__file__).parent
        self.model = "gpt-4o-mini"
        self.temperature = 0.1
        self.data = self.load_json(script_dir / "resources/uia_log.json")
        self.examples = self.load_json(script_dir / "resources/listener_data_examples.json")
        self.actions_formats = self.load_text(script_dir / "resources/format_templates.md")
        # self.schema_requirements = self.load_text(script_dir / "resources/schema_requirements.md")

    @staticmethod
    def load_json(file_path: str) -> Dict[str, Any]:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"Warning: {file_path} not found")
            return {}
        except json.JSONDecodeError:
            print(f"Warning: {file_path} contains invalid JSON")
            return {}

    @staticmethod
    def load_text(file_path: str) -> str:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"Warning: {file_path} not found")
            return ""

    @staticmethod
    def _escape_braces(s: str) -> str:
        return s.replace("{", "{{").replace("}", "}}")
    
    def generate_schema_prompt(self) -> Optional[Dict[str, Any]]:
        """
        Build the prompt and call the LLM. Expects run_request to return a dict like:
        {"success": bool, "prompt": str, "schema": dict} OR {"success": bool, "output_json": {...}}
        """
        try:
            prompt = JSON_SCHEMA_PROMPT.format (
            bot_listener_data=json.dumps(self.data, indent=2),
            listener_data_examples=json.dumps(self.examples, indent=2),
            actions_formats=self.actions_formats,
            # schema_requirements=self._escape_braces(self.schema_requirements),
            )

            # Optional: avoid dumping giant prompt bodies to stdout
            print(f"Prompt built. Length: {len(prompt)} chars")

            result = run_request(
                prompt=prompt,
                model=self.model,
                temperature=self.temperature,
                max_tokens=4000,
            )
            
            # Save the result in a file (only if it's serializable)
            try:
                with open("result.json", "w") as f:
                    json.dump(result, f, indent=2, default=str)
                print("üìÑ Result saved to result.json")
            except (TypeError, ValueError) as e:
                print(f"‚ö†Ô∏è Could not save result to JSON: {e}")
                # Save as string representation instead
                with open("result.txt", "w") as f:
                    f.write(str(result))
                print("üìÑ Result saved to result.txt as string")
            
            return result
        except Exception as e:
            print(f"‚ùå Error calling OpenAI API: {e}")
            return None


def main():
    print("üöÄ Starting JSON Schema Generation...")
    engine = UIA_Listener_Data()              # loads resources in __init__
    result = engine.generate_schema_prompt()  # calls the LLM

    if not result or not result.get("success"):
        print("‚ùå Process failed")
        if result and result.get("error"):
            print(f"Error: {result['error']}")
        return

    print("üéâ Success")
    
    # Extract and save the schema
    schema_obj = None
    
    # Try to extract schema from different possible locations in the result
    if isinstance(result.get("schema"), dict):
        schema_obj = result["schema"]
    elif isinstance(result.get("output_json"), dict):
        output = result["output_json"]
        if isinstance(output.get("schema"), dict):
            schema_obj = output["schema"]
    elif result.get("content"):
        # The schema might be in the content field as a JSON string
        content_str = result["content"]
        
        # Handle markdown code blocks if present
        if content_str.startswith("```") and content_str.endswith("```"):
            # Extract content between code blocks
            lines = content_str.split('\n')
            if len(lines) >= 3:
                # Skip first line (```json) and last line (```)
                json_content = '\n'.join(lines[1:-1])
                try:
                    content = json.loads(json_content)
                    if isinstance(content.get("schema"), dict):
                        schema_obj = content["schema"]
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"‚ö†Ô∏è Could not parse extracted JSON content: {e}")
                    print(f"Extracted content preview: {json_content[:200]}...")
        else:
            # Try to parse the content directly as JSON
            try:
                content = json.loads(content_str)
                if isinstance(content.get("schema"), dict):
                    schema_obj = content["schema"]
            except (json.JSONDecodeError, TypeError) as e:
                print(f"‚ö†Ô∏è Could not parse content as JSON: {e}")
                print(f"Content preview: {content_str[:200]}...")
    
    if schema_obj:
        # Save the schema to a file
        script_dir = Path(__file__).parent
        schema_file = script_dir / "resources/generated_schema.json"
        with open(schema_file, 'w', encoding='utf-8') as f:
            json.dump(schema_obj, f, indent=2)
        print(f"üìã Schema saved to: {schema_file}")
        
        # Show a preview
        preview = json.dumps(schema_obj, indent=2)
        print("\nüìã Schema Preview:")
        print(preview[:1000] + ("..." if len(preview) > 1000 else ""))
    else:
        print("‚ùå No schema was found in the result")
        print("Available keys in result:", list(result.keys()) if result else "None")
        if result and "output_json" in result:
            print("Available keys in output_json:", list(result["output_json"].keys()) if isinstance(result["output_json"], dict) else "Not a dict")
        if result and "content" in result:
            print("Content field contains:", type(result["content"]), "with length:", len(str(result["content"])))


if __name__ == "__main__":
    main()